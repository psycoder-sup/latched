{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "import os\n",
            "\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "from transformers.models.mistral.modeling_mistral import (\n",
            "    MISTRAL_ATTENTION_CLASSES,\n",
            "    MistralAttention,\n",
            "    MistralConfig,\n",
            "    MistralForCausalLM,\n",
            "    apply_rotary_pos_emb,\n",
            "    repeat_kv\n",
            "    )\n",
            "from transformers.cache_utils import Cache\n",
            "\n",
            "from typing import Tuple, List, Optional\n",
            "\n",
            "import torch"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "class SliceUpdateKeyValueCache(Cache):\n",
            "    def __init__(\n",
            "        self,\n",
            "        shape: Tuple[int, ...],\n",
            "        device: str = \"cpu\",\n",
            "        dtype=torch.float32\n",
            "    ) -> None:\n",
            "        super().__init__()\n",
            "        self.past_seen_tokens: int = 0\n",
            "        self.k_cache: torch.Tensor = torch.zeros(shape, dtype=dtype, device=device)\n",
            "        self.v_cache: torch.Tensor = torch.zeros(shape, dtype=dtype, device=device)\n",
            "\n",
            "    def update(\n",
            "        self,\n",
            "        k_state: torch.Tensor,\n",
            "        v_state: torch.Tensor,\n",
            "        layer_idx: int,\n",
            "        slice_indices: torch.LongTensor\n",
            "        ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
            "        \n",
            "        if len(slice_indices) != 2:\n",
            "            raise ValueError(f\"slice_indices must be of length 2, got {len(slice_indices)}\")\n",
            "        begin, end = slice_indices\n",
            "        self.k_cache[layer_idx, :, : k_state.shape[1], begin: end, :] = k_state\n",
            "        self.v_cache[layer_idx, :, : v_state.shape[1], begin: end, :] = v_state\n",
            "        k_cache: torch.Tensor = self.k_cache[layer_idx, :, :, :end, :]\n",
            "        v_cache: torch.Tensor = self.v_cache[layer_idx, :, :, :end, :]\n",
            "        return k_cache, v_cache\n",
            "    \n",
            "    def get_seq_length(self, _: int | None = 0) -> int:\n",
            "        return self.past_seen_tokens\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "class SliceUpdateMistralAttention(MistralAttention):\n",
            "    def __init__(self, config: MistralConfig, layer_idx: Optional[int] = None) -> None:\n",
            "        super().__init__(config, layer_idx)\n",
            "        \n",
            "    @torch.no_grad()\n",
            "    def forward(\n",
            "        self,\n",
            "        hidden_states: torch.Tensor,\n",
            "        attention_mask: torch.Tensor,\n",
            "        position_ids: Optional[torch.Tensor] = None,\n",
            "        past_key_value: Optional[SliceUpdateKeyValueCache] = None,\n",
            "        **kwargs\n",
            "    ) -> Tuple[torch.Tensor | None, ...]:\n",
            "        bsz, q_len, _ = hidden_states.shape\n",
            "        \n",
            "        query_states = self.q_proj(hidden_states)\n",
            "        key_states = self.k_proj(hidden_states)\n",
            "        value_states = self.v_proj(hidden_states)\n",
            "\n",
            "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
            "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(\n",
            "            1, 2\n",
            "        )\n",
            "        value_states = value_states.view(\n",
            "            bsz, q_len, self.num_key_value_heads, self.head_dim\n",
            "        ).transpose(1, 2)\n",
            "        \n",
            "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
            "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
            "        \n",
            "        end_step = attention_mask.shape[-1]\n",
            "        key_states, value_states = past_key_value.update(\n",
            "            key_states,\n",
            "            value_states,\n",
            "            self.layer_idx,\n",
            "            slice_indices=(end_step - q_len, end_step)\n",
            "        )\n",
            "        \n",
            "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
            "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
            "        \n",
            "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "            query_states,\n",
            "            key_states,\n",
            "            value_states,\n",
            "            attn_mask = attention_mask\n",
            "        )\n",
            "        \n",
            "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
            "        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n",
            "        attn_output = self.o_proj(attn_output)\n",
            "        return attn_output, None, None\n",
            "        \n",
            "            \n",
            "            "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [],
         "source": [
            "class StatefulMistralModelForCausalLM(torch.nn.Module):\n",
            "    def __init__(self, model_path: str, max_context_size: int = 2048, batch_size: int = 1):\n",
            "        super().__init__()\n",
            "        # MISTRAL_ATTENTION_CLASSES[\"sdpa\"] = SliceUpdateMistralAttention\n",
            "        self.model = MistralForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
            "        \n",
            "        # config: MistralConfig = self.model.config\n",
            "        # self.kv_cache_shape: Tuple[int, ...] = (\n",
            "        #     config.num_hidden_layers,\n",
            "        #     batch_size,\n",
            "        #     config.num_key_value_heads,\n",
            "        #     max_context_size,\n",
            "        #     config.hidden_size // config.num_attention_heads\n",
            "        # )\n",
            "        # self.kv_cache = SliceUpdateKeyValueCache(shape=self.kv_cache_shape)\n",
            "        # self.register_buffer(\"keyCache\", self.kv_cache.k_cache)\n",
            "        # self.register_buffer(\"valueCache\", self.kv_cache.v_cache)\n",
            "        \n",
            "    @torch.no_grad()\n",
            "    def forward(\n",
            "        self,\n",
            "        input_ids: torch.LongTensor,\n",
            "        causal_mask: torch.Tensor\n",
            "    ) -> torch.Tensor:\n",
            "        # self.kv_cache.past_seen_tokens = causal_mask.shape[-1] - input_ids.shape[-1]\n",
            "        return self.model(\n",
            "            input_ids,\n",
            "            attention_mask=causal_mask,\n",
            "            # past_key_values=self.kv_cache\n",
            "        ).logits\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "application/vnd.jupyter.widget-view+json": {
                     "model_id": "9bffe4e034a340b2967818bd1f8438f0",
                     "version_major": 2,
                     "version_minor": 0
                  },
                  "text/plain": [
                     "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/modeling_utils.py:4674: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
                  "  warnings.warn(\n",
                  "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
                  "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:910: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
                  "  if attention_mask.max() != 0:\n"
               ]
            }
         ],
         "source": [
            "max_context_size: int = 2048\n",
            "model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
            "torch_model = StatefulMistralModelForCausalLM(model_id, max_context_size=max_context_size)\n",
            "torch_model.eval()\n",
            "input_ids: torch.Tensor = torch.zeros((1,2), dtype=torch.int32)\n",
            "causal_mask: torch.Tensor = torch.zeros((1, 1, 2, 5), dtype=torch.float16)\n",
            "traced_model = torch.jit.trace(torch_model, [input_ids, causal_mask])\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "# save traced model\n",
            "\n",
            "traced_model.save(\"mistral-7B-fp16-traced.pt\") # type: ignore"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "tokenizer = AutoTokenizer.from_pretrained(model_id)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "{'input_ids': tensor([[    1, 23325, 29493,  1678,  1228,  1136, 29572]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
               ]
            }
         ],
         "source": [
            "test_input = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
            "print(test_input)\n",
            "\n",
            "causal_mask = torch.triu(torch.full((1, 1, test_input.input_ids.shape[-1], test_input.input_ids.shape[-1]), 0, dtype=torch.float16))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "output = traced_model(test_input.input_ids, causal_mask) # type: ignore"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [],
         "source": [
            "def sample(logits: torch.Tensor, k: int = 3) -> torch.Tensor:\n",
            "    # Get the last token's logits\n",
            "    last_token_logits = logits[0][-1]\n",
            "    # Get the top k values and indices\n",
            "    top_k_values, top_k_indices = torch.topk(last_token_logits, k)\n",
            "    # Sample from the top k\n",
            "    probs = torch.softmax(top_k_values, dim=-1)\n",
            "    chosen_idx = torch.multinomial(probs, num_samples=1, generator=torch.Generator(device=\"cuda\").manual_seed(42))\n",
            "    return top_k_indices[chosen_idx]\n",
            "\n",
            "from typing import Generator\n",
            "\n",
            "def generate(model: torch.nn.Module, input_ids: torch.Tensor, eos_id: int, max_length: int, device: str = \"cpu\") -> Generator[torch.Tensor, None, None]:\n",
            "    model.to(device)\n",
            "    def inference(model: torch.nn.Module, input_ids: torch.Tensor) -> torch.Tensor:\n",
            "        input_ids = input_ids.to(device)\n",
            "        causal_mask = torch.triu(torch.full((1, 1, input_ids.shape[-1], input_ids.shape[-1]), 0, dtype=torch.float16), diagonal=1).to(device)\n",
            "        return model(input_ids, causal_mask)\n",
            "    input_ids = input_ids.to(device)\n",
            "    logits = inference(model, input_ids)\n",
            "    token = sample(logits)\n",
            "    n_tokens = 0\n",
            "    while True:\n",
            "        yield token\n",
            "        # print(token.shape)\n",
            "        input_ids = torch.cat([input_ids, token.unsqueeze(0)], dim=-1)\n",
            "        logits = inference(model, input_ids)\n",
            "        token = sample(logits)\n",
            "        if token == eos_id or n_tokens >= max_length:\n",
            "            break\n",
            "        n_tokens += 1\n",
            "    "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "I ' m doing well , thank you . How can I help you today ? I ' m looking for a specific book . Great ! I ' d be happy to help you find it . Could you please tell me the title and author of the book you ' re looking for ? Sure , the book is called \" To Kill a Mock ing bird \" and it was written by Harper Lee . I ' ll see if I can find it for you . \n",
                  " \n",
                  " I ' m sorry , but I don ' "
               ]
            }
         ],
         "source": [
            "token_output = []\n",
            "for token in generate(torch_model, test_input.input_ids, tokenizer.eos_token_id, 100, device=\"cuda\"):\n",
            "    token_output.append(token)\n",
            "    print(tokenizer.decode(token), end=\" \")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 70,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}"
                  ]
               },
               "execution_count": 70,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": 23,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "tensor(1083)\n",
                  "torch.Size([])\n",
                  "torch.Size([1, 7, 32768])\n",
                  "torch.Size([1, 7])\n"
               ]
            }
         ],
         "source": [
            "sampled_token = sample(output)\n",
            "print(sampled_token)\n",
            "\n",
            "tokenizer.decode(sampled_token)\n",
            "print(sampled_token.shape)\n",
            "print(output.shape)\n",
            "print(test_input.input_ids.shape)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n"
               ]
            }
         ],
         "source": [
            "import coremltools as ct\n",
            "import numpy as np\n",
            "# convert traced TorchScript to CoreML format\n",
            "\n",
            "query_length = ct.RangeDim(lower_bound=1, upper_bound=max_context_size, default=1)\n",
            "end_step_dim = ct.RangeDim(lower_bound=1, upper_bound=max_context_size, default=1)\n",
            "\n",
            "inputs: List[ct.TensorType] = [\n",
            "    ct.TensorType(shape=(1, query_length), dtype=np.int32, name=\"inputIds\"),\n",
            "    ct.TensorType(\n",
            "        shape=(1, 1, query_length, end_step_dim),\n",
            "        dtype=np.float16,\n",
            "        name=\"causalMask\"\n",
            "    )\n",
            "]\n",
            "\n",
            "outputs: List[ct.TensorType] = [ct.TensorType(dtype=np.float16, name=\"logits\")]\n",
            "# states: List[ct.StateType] = [\n",
            "#     ct.StateType(\n",
            "#         wrapped_type=ct.TensorType(shape=torch_model.kv_cache_shape, dtype=np.float16),\n",
            "#         name=\"keyCache\"\n",
            "#     ),\n",
            "#     ct.StateType(\n",
            "#         wrapped_type=ct.TensorType(shape=torch_model.kv_cache_shape, dtype=np.float16),\n",
            "#         name=\"valueCache\"\n",
            "#     )\n",
            "# ]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 4740/4741 [00:02<00:00, 1851.24 ops/s]\n",
                  "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00,  8.00 passes/s]\n",
                  "Running MIL default pipeline: 100%|██████████| 84/84 [00:20<00:00,  4.16 passes/s]\n",
                  "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 24.88 passes/s]\n"
               ]
            }
         ],
         "source": [
            "mlmodel_fp16 = ct.convert(\n",
            "    traced_model,\n",
            "    inputs=inputs,\n",
            "    outputs=outputs,\n",
            "    # states=states,\n",
            "    minimum_deployment_target=ct.target.iOS18,\n",
            "    skip_model_load=True,\n",
            "    compute_precision=ct.precision.FLOAT16\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [],
         "source": [
            "mlmodel_fp16.save(\"mlmodel-no-state-fp32.mlpackage\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Running compression pass linear_quantize_weights: 100%|██████████| 296/296 [00:44<00:00,  6.72 ops/s]\n",
                  "Running MIL frontend_milinternal pipeline: 0 passes [00:00, ? passes/s]\n",
                  "Running MIL default pipeline: 100%|██████████| 84/84 [00:12<00:00,  6.92 passes/s]\n",
                  "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 20.78 passes/s]\n"
               ]
            }
         ],
         "source": [
            "op_config = ct.optimize.coreml.OpLinearQuantizerConfig(\n",
            "    mode=\"linear_symmetric\",\n",
            "    dtype=\"int4\",\n",
            "    granularity=\"per_block\",\n",
            "    block_size=32    \n",
            ")\n",
            "\n",
            "config = ct.optimize.coreml.OptimizationConfig(global_config=op_config)\n",
            "mlmodel_int4 = ct.optimize.coreml.linear_quantize_weights(mlmodel_fp16, config=config)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [],
         "source": [
            "mlmodel_int4.save(\"mlmodel-no-state-fp32-to-int4.mlpackage\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 18,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  " 14G\t./mlmodel-no-state-fp16.mlpackage/\n"
               ]
            }
         ],
         "source": [
            "!du -hs ./mlmodel-no-state-fp16.mlpackage/"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 19,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "3.8G\t./mlmodel-no-state-int4.mlpackage/\n"
               ]
            }
         ],
         "source": [
            "!du -hs ./mlmodel-no-state-int4.mlpackage/"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "latched",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.9"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
