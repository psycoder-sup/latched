{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9a070cbad640a2911cc62ae29a23cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805359d726d8460ca772c40cb160e717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d6bdf435fa41cb87c2a8d5947d9706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05c9b5abeda4457997232e28f135d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45840906884a4a5ba1f5669d65369133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4228d98158504c43b2f2e639cb580694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3daaa4e8af04f10a536bb3334b2a7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f97abbd8c64e9ab5789ad3570a7903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f270eb948644c9ea6a61ed3fb9df9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f12444e270a4c63a01b72744a877add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2f87c6f13f4c42b71f7bf0cf9bd46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4338d1323e54403ab995c05868a6e10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Wrapper Class\n",
    "class MistralModelForCausalLM(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(input_ids, attention_mask).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1, 23325, 29493,  1678,  1228,  1136, 29572]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# test inference\n",
    "\n",
    "test_input = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "print(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    }
   ],
   "source": [
    "model = MistralModelForCausalLM(model).eval()\n",
    "\n",
    "output = model(**test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids: torch.Tensor = torch.zeros((1, 2), dtype=torch.int32)\n",
    "causal_mask: torch.Tensor = torch.zeros((1, 1, 2, 5), dtype=torch.float32)\n",
    "\n",
    "output = model.forward(input_ids, causal_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.3285, -8.7385,  1.8744,  ..., -7.1242, -4.3918, -8.2920],\n",
       "         [-9.3285, -8.7385,  1.8744,  ..., -7.1242, -4.3918, -8.2920]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = model.forward(input_ids, torch.zeros((1,2), dtype=torch.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.mistral.modeling_mistral import (\n",
    "    MISTRAL_ATTENTION_CLASSES,\n",
    "    MistralAttention,\n",
    "    MistralConfig,\n",
    "    MistralForCausalLM,\n",
    "    apply_rotary_pos_emb,\n",
    "    repeat_kv\n",
    "    )\n",
    "from transformers.cache_utils import Cache\n",
    "\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceUpdateKeyValueCache(Cache):\n",
    "    def __init__(\n",
    "        self,\n",
    "        shape: Tuple[int, ...],\n",
    "        device: str = \"cpu\",\n",
    "        dtype=torch.float32\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.past_seen_tokens: int = 0\n",
    "        self.k_cache: torch.Tensor = torch.zeros(shape, dtype=dtype, device=device)\n",
    "        self.v_cache: torch.Tensor = torch.zeros(shape, dtype=dtype, device=device)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        k_state: torch.Tensor,\n",
    "        v_state: torch.Tensor,\n",
    "        layer_idx: int,\n",
    "        slice_indices: torch.LongTensor\n",
    "        ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        if len(slice_indices) != 2:\n",
    "            raise ValueError(f\"slice_indices must be of length 2, got {len(slice_indices)}\")\n",
    "        begin, end = slice_indices\n",
    "        self.k_cache[layer_idx, :, : k_state.shape[1], begin: end, :] = k_state\n",
    "        self.v_cache[layer_idx, :, : v_state.shape[1], begin: end, :] = v_state\n",
    "        k_cache: torch.Tensor = self.k_cache[layer_idx, :, :, :end, :]\n",
    "        v_cache: torch.Tensor = self.v_cache[layer_idx, :, :, :end, :]\n",
    "        return k_cache, v_cache\n",
    "    \n",
    "    def get_seq_length(self, _: int | None = 0) -> int:\n",
    "        return self.past_seen_tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SliceUpdateMistralAttention(MistralAttention):\n",
    "    def __init__(self, config: MistralConfig, layer_idx: Optional[int] = None) -> None:\n",
    "        super().__init__(config, layer_idx)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        **kwargs\n",
    "    ) -> Tuple[torch.Tensor | None, ...]:\n",
    "        bsz, q_len, _ = hidden_states.shape\n",
    "        \n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        value_states = value_states.view(\n",
    "            bsz, q_len, self.num_key_value_heads, self.head_dim\n",
    "        ).transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        \n",
    "        end_step = attention_mask.shape[-1]\n",
    "        key_states, value_states = past_key_value.update(\n",
    "            key_states,\n",
    "            value_states,\n",
    "            self.layer_idx,\n",
    "            slice_indices=(end_step - q_len, end_step)\n",
    "        )\n",
    "        \n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        \n",
    "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attn_mask = attention_mask\n",
    "        )\n",
    "        \n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, None, None\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulMistralModelForCausalLM(torch.nn.Module):\n",
    "    def __init__(self, model_path: str, max_context_size: int = 2048, batch_size: int = 1):\n",
    "        super().__init__()\n",
    "        # MISTRAL_ATTENTION_CLASSES[\"sdpa\"] = SliceUpdateMistralAttention\n",
    "        self.model = MistralForCausalLM.from_pretrained(model_path)\n",
    "        \n",
    "        # config: MistralConfig = self.model.config\n",
    "        # self.kv_cache_shape: Tuple[int, ...] = (\n",
    "        #     config.num_hidden_layers,\n",
    "        #     batch_size,\n",
    "        #     config.num_key_value_heads,\n",
    "        #     max_context_size,\n",
    "        #     config.hidden_size // config.num_attention_heads\n",
    "        # )\n",
    "        # self.kv_cache = SliceUpdateKeyValueCache(shape=self.kv_cache_shape)\n",
    "        # self.register_buffer(\"keyCache\", self.kv_cache.k_cache)\n",
    "        # self.register_buffer(\"valueCache\", self.kv_cache.v_cache)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        causal_mask: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # self.kv_cache.past_seen_tokens = causal_mask.shape[-1] - input_ids.shape[-1]\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=causal_mask,\n",
    "            # past_key_values=self.kv_cache\n",
    "        ).logits\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee3af31a75b4201afef04434d412a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/modeling_utils.py:4674: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:910: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.max() != 0:\n"
     ]
    }
   ],
   "source": [
    "max_context_size: int = 2048\n",
    "model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "torch_model = StatefulMistralModelForCausalLM(model_id, max_context_size=max_context_size)\n",
    "torch_model.eval()\n",
    "input_ids: torch.Tensor = torch.zeros((1,2), dtype=torch.int32)\n",
    "causal_mask: torch.Tensor = torch.zeros((1, 1, 2, 5), dtype=torch.float32)\n",
    "traced_model = torch.jit.trace(torch_model, [input_ids, causal_mask])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "import numpy as np\n",
    "# convert traced TorchScript to CoreML format\n",
    "\n",
    "query_length = ct.RangeDim(lower_bound=1, upper_bound=max_context_size, default=1)\n",
    "end_step_dim = ct.RangeDim(lower_bound=1, upper_bound=max_context_size, default=1)\n",
    "\n",
    "inputs: List[ct.TensorType] = [\n",
    "    ct.TensorType(shape=(1, query_length), dtype=np.int32, name=\"inputIds\"),\n",
    "    ct.TensorType(\n",
    "        shape=(1, 1, query_length, end_step_dim),\n",
    "        dtype=np.float16,\n",
    "        name=\"causal_mask\"\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs: List[ct.TensorType] = [ct.TensorType(dtype=np.float16, name=\"logits\")]\n",
    "# states: List[ct.StateType] = [\n",
    "#     ct.StateType(\n",
    "#         wrapped_type=ct.TensorType(shape=torch_model.kv_cache_shape, dtype=np.float16),\n",
    "#         name=\"keyCache\"\n",
    "#     ),\n",
    "#     ct.StateType(\n",
    "#         wrapped_type=ct.TensorType(shape=torch_model.kv_cache_shape, dtype=np.float16),\n",
    "#         name=\"valueCache\"\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 4740/4741 [00:02<00:00, 2264.50 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00,  7.38 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [02:54<00:00,  2.02s/ passes]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 21.16 passes/s]\n"
     ]
    }
   ],
   "source": [
    "mlmodel_fp16 = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    # states=states,\n",
    "    minimum_deployment_target=ct.target.iOS18,\n",
    "    skip_model_load=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_fp16.save(\"mlmodel-no-state-fp16.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running compression pass linear_quantize_weights:   0%|          | 0/296 [00:00<?, ? ops/s]/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/coremltools/optimize/coreml/_utils.py:87: RuntimeWarning: invalid value encountered in divide\n",
      "  quantized_data = np.round(weight / scale)\n",
      "/home/sanguk/miniconda3/envs/latched/lib/python3.11/site-packages/coremltools/optimize/coreml/_utils.py:91: RuntimeWarning: invalid value encountered in cast\n",
      "  quantized_data = np.clip(quantized_data, q_val_min, q_val_max).astype(dtype)\n",
      "Running compression pass linear_quantize_weights: 100%|██████████| 296/296 [05:20<00:00,  1.08s/ ops]\n",
      "Running MIL frontend_milinternal pipeline: 0 passes [00:00, ? passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 84/84 [00:14<00:00,  5.71 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 17.64 passes/s]\n"
     ]
    }
   ],
   "source": [
    "op_config = ct.optimize.coreml.OpLinearQuantizerConfig(\n",
    "    mode=\"linear_symmetric\",\n",
    "    dtype=\"int4\",\n",
    "    granularity=\"per_block\",\n",
    "    block_size=32    \n",
    ")\n",
    "\n",
    "config = ct.optimize.coreml.OptimizationConfig(global_config=op_config)\n",
    "mlmodel_int4 = ct.optimize.coreml.linear_quantize_weights(mlmodel_fp16, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_int4.save(\"mlmodel-no-state-int4.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14G\t./mlmodel-no-state-fp16.mlpackage/\n"
     ]
    }
   ],
   "source": [
    "!du -hs ./mlmodel-no-state-fp16.mlpackage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8G\t./mlmodel-no-state-int4.mlpackage/\n"
     ]
    }
   ],
   "source": [
    "!du -hs ./mlmodel-no-state-int4.mlpackage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latched",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
